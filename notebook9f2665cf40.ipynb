{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-13T13:25:53.050159Z","iopub.execute_input":"2024-05-13T13:25:53.050741Z","iopub.status.idle":"2024-05-13T13:25:53.606255Z","shell.execute_reply.started":"2024-05-13T13:25:53.050699Z","shell.execute_reply":"2024-05-13T13:25:53.605007Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\n/kaggle/input/home-credit-credit-risk-model-stability/feature_definitions.csv\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_deposit_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_cb_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_0_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_3.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_tax_registry_b_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_0_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_3.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_9.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_debitcard_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_11.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_1_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_4.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_tax_registry_c_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_1_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_tax_registry_a_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_6.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_5.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_b_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_other_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_0_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_1_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_base.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_person_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_7.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_10.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_b_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_8.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_4.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_person_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_tax_registry_c_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_static_0_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_3.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_b_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_applprev_1_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_static_cb_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_other_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_6.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_tax_registry_a_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_tax_registry_b_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_person_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_person_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_b_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_7.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_deposit_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_debitcard_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_5.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_4.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_base.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_9.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_3.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_applprev_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_10.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_8.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_2.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_static_0_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_1.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_applprev_1_0.parquet\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_6.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_11.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_9.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_base.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_b_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_10.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_4.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_0_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_cb_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_b_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_tax_registry_b_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_person_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_person_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_8.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_1_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_1_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_7.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_other_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_tax_registry_c_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_3.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_0_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_5.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_debitcard_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_4.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_deposit_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_0_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_1_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_tax_registry_a_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_3.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_3.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_static_cb_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_person_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_base.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_a_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_static_0_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_6.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_person_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_c_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_4.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_9.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_3.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_7.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_b_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_2.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_static_0_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_deposit_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_10.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_b_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_8.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_5.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_b_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_0.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_other_1.csv\n/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_debitcard_1.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:53.608773Z","iopub.execute_input":"2024-05-13T13:25:53.609297Z","iopub.status.idle":"2024-05-13T13:25:54.517519Z","shell.execute_reply.started":"2024-05-13T13:25:53.609261Z","shell.execute_reply":"2024-05-13T13:25:54.516288Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n# from woe_conversion.woe import *\nfrom sklearn.impute import KNNImputer\n\nfrom imblearn.combine import SMOTEENN\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:54.519080Z","iopub.execute_input":"2024-05-13T13:25:54.519551Z","iopub.status.idle":"2024-05-13T13:25:56.549575Z","shell.execute_reply.started":"2024-05-13T13:25:54.519509Z","shell.execute_reply":"2024-05-13T13:25:56.548390Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.551033Z","iopub.execute_input":"2024-05-13T13:25:56.551758Z","iopub.status.idle":"2024-05-13T13:25:56.702427Z","shell.execute_reply.started":"2024-05-13T13:25:56.551714Z","shell.execute_reply":"2024-05-13T13:25:56.701256Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTRAIN_CSV_DIR   = ROOT / \"csv_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.706410Z","iopub.execute_input":"2024-05-13T13:25:56.707295Z","iopub.status.idle":"2024-05-13T13:25:56.714008Z","shell.execute_reply.started":"2024-05-13T13:25:56.707260Z","shell.execute_reply":"2024-05-13T13:25:56.712760Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import threading\n\ndef process_files(filename, dir, feature_sheet_list):\n    '''\n    处理单个csv\n    '''\n    df = pd.read_parquet(os.path.join(dir, filename))\n    print('Processing: ', filename)\n\n    info = pd.DataFrame(df.isnull().sum(), columns=['nan_num'])\n    info['df_len'] = df.shape[0]\n    info['file'] = filename\n    info = info.rename(index={'case_id': f'case_id_{filename[:-4]}'})\n    \n    feature_sheet_list.append(info)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.715554Z","iopub.execute_input":"2024-05-13T13:25:56.716027Z","iopub.status.idle":"2024-05-13T13:25:56.729448Z","shell.execute_reply.started":"2024-05-13T13:25:56.715965Z","shell.execute_reply":"2024-05-13T13:25:56.728242Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def feature_sheet_process(file_list, dir):\n    '''\n    多线程处理\n    '''\n    threads = []\n    feature_sheet_list = []\n\n    for file in file_list:\n        thread = threading.Thread(target=process_files, args=(file, dir, feature_sheet_list))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    combined_feature_sheet = pd.concat(feature_sheet_list, axis=0)\n    return combined_feature_sheet","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.731138Z","iopub.execute_input":"2024-05-13T13:25:56.731697Z","iopub.status.idle":"2024-05-13T13:25:56.745781Z","shell.execute_reply.started":"2024-05-13T13:25:56.731652Z","shell.execute_reply":"2024-05-13T13:25:56.744294Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"'''\ndata_store = {}\nfor i in range(0, 32, 8):\n    train_feature_sheet = feature_sheet_process(train_list[i:i+8], TRAIN_DIR)\n    data_store.update({f'feature_sheet_{i//8}': train_feature_sheet})\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.747536Z","iopub.execute_input":"2024-05-13T13:25:56.747956Z","iopub.status.idle":"2024-05-13T13:25:56.761336Z","shell.execute_reply.started":"2024-05-13T13:25:56.747923Z","shell.execute_reply":"2024-05-13T13:25:56.760068Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"\\ndata_store = {}\\nfor i in range(0, 32, 8):\\n    train_feature_sheet = feature_sheet_process(train_list[i:i+8], TRAIN_DIR)\\n    data_store.update({f'feature_sheet_{i//8}': train_feature_sheet})\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\ntrain_feature_sheet = data_store['feature_sheet_0.csv']\nfor i in range(1,4):\n    train_feature_sheet = pd.concat([train_feature_sheet, data_store[f'feature_sheet_{i}.csv']], axis=0)\ndel data_store\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.762676Z","iopub.execute_input":"2024-05-13T13:25:56.763080Z","iopub.status.idle":"2024-05-13T13:25:56.775589Z","shell.execute_reply.started":"2024-05-13T13:25:56.763033Z","shell.execute_reply":"2024-05-13T13:25:56.774446Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"\\ntrain_feature_sheet = data_store['feature_sheet_0.csv']\\nfor i in range(1,4):\\n    train_feature_sheet = pd.concat([train_feature_sheet, data_store[f'feature_sheet_{i}.csv']], axis=0)\\ndel data_store\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\ntrain_feature_sheet = train_feature_sheet.rename(columns={'Unnamed: 0': 'feature'})\ntrain_feature_sheet\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.777015Z","iopub.execute_input":"2024-05-13T13:25:56.777420Z","iopub.status.idle":"2024-05-13T13:25:56.788466Z","shell.execute_reply.started":"2024-05-13T13:25:56.777374Z","shell.execute_reply":"2024-05-13T13:25:56.787250Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"\\ntrain_feature_sheet = train_feature_sheet.rename(columns={'Unnamed: 0': 'feature'})\\ntrain_feature_sheet\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\ntrain_feature_sheet = train_feature_sheet.groupby('feature').agg({'nan_num': 'sum', 'df_len': 'sum', 'file': list}).reset_index()\ntrain_feature_sheet['nan_ratio'] = train_feature_sheet['nan_num'] / train_feature_sheet['df_len']\ntrain_feature_sheet\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.789970Z","iopub.execute_input":"2024-05-13T13:25:56.790467Z","iopub.status.idle":"2024-05-13T13:25:56.804899Z","shell.execute_reply.started":"2024-05-13T13:25:56.790434Z","shell.execute_reply":"2024-05-13T13:25:56.803591Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"\"\\ntrain_feature_sheet = train_feature_sheet.groupby('feature').agg({'nan_num': 'sum', 'df_len': 'sum', 'file': list}).reset_index()\\ntrain_feature_sheet['nan_ratio'] = train_feature_sheet['nan_num'] / train_feature_sheet['df_len']\\ntrain_feature_sheet\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\n# 按照feature首字母排序\ntrain_feature_sheet = train_feature_sheet.sort_values(by='feature')\ntrain_feature_sheet\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.806513Z","iopub.execute_input":"2024-05-13T13:25:56.806979Z","iopub.status.idle":"2024-05-13T13:25:56.818987Z","shell.execute_reply.started":"2024-05-13T13:25:56.806935Z","shell.execute_reply":"2024-05-13T13:25:56.817416Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"\"\\n# 按照feature首字母排序\\ntrain_feature_sheet = train_feature_sheet.sort_values(by='feature')\\ntrain_feature_sheet\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\n# train_feature_sheet丢弃feature为'case_id'开头的行\ntrain_feature_sheet_1 = train_feature_sheet[~train_feature_sheet['feature'].str.startswith('case_id')]\ndel train_feature_sheet\ntrain_feature_sheet_1\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.820578Z","iopub.execute_input":"2024-05-13T13:25:56.820972Z","iopub.status.idle":"2024-05-13T13:25:56.833489Z","shell.execute_reply.started":"2024-05-13T13:25:56.820941Z","shell.execute_reply":"2024-05-13T13:25:56.832356Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"\\n# train_feature_sheet丢弃feature为'case_id'开头的行\\ntrain_feature_sheet_1 = train_feature_sheet[~train_feature_sheet['feature'].str.startswith('case_id')]\\ndel train_feature_sheet\\ntrain_feature_sheet_1\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\n# feature_definitions里面的feature\nfeature_list = pd.read_csv(os.path.joint(ROOT, 'feature_definitions.csv'))\nfeatures1 = feature_list['Variable'].values\nfeatures0 = train_feature_sheet_1['feature'].values\nfeatures = list(set(features0) - set(features1))\nfeatures  # 在csv不在definitions里面的feature(就是train_base里面的)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.838736Z","iopub.execute_input":"2024-05-13T13:25:56.839503Z","iopub.status.idle":"2024-05-13T13:25:56.854318Z","shell.execute_reply.started":"2024-05-13T13:25:56.839459Z","shell.execute_reply":"2024-05-13T13:25:56.853046Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"\\n# feature_definitions里面的feature\\nfeature_list = pd.read_csv(os.path.joint(ROOT, 'feature_definitions.csv'))\\nfeatures1 = feature_list['Variable'].values\\nfeatures0 = train_feature_sheet_1['feature'].values\\nfeatures = list(set(features0) - set(features1))\\nfeatures  # 在csv不在definitions里面的feature(就是train_base里面的)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''\n# 保留feature_list中Variable列的值在train_feature_sheet_1中的行\ntrain_feature_sheet_2 = train_feature_sheet_1[train_feature_sheet_1['feature'].isin(features1)]\nfeature_info = pl.DataFrame(train_feature_sheet_2.reset_index())\ndel train_feature_sheet_2, train_feature_sheet_1, feature0, features, feature1, feature_list\n\nwith open(\"feature_info.pkl\", 'wb') as f:\n    pickle.dump(feature_info, f)\n\ndir = 'home-credit-credit-risk-model-stability'\nfeature_info = pl.read_csv(\"feature_info.csv\")\nfeature_info = feature_info.filter(pl.col(\"nan_ratio\") <= 0.7)\nfeat_defs = pl.read_csv(os.path.join(dir0, 'feature_definitions.csv'))\nfeat_defs = feat_defs.filter(pl.col(\"Variable\").is_in(feature_info[\"feature\"]))\ndel dir, feature_info, feat_defs\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.855689Z","iopub.execute_input":"2024-05-13T13:25:56.856061Z","iopub.status.idle":"2024-05-13T13:25:56.868250Z","shell.execute_reply.started":"2024-05-13T13:25:56.856025Z","shell.execute_reply":"2024-05-13T13:25:56.867065Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'\\n# 保留feature_list中Variable列的值在train_feature_sheet_1中的行\\ntrain_feature_sheet_2 = train_feature_sheet_1[train_feature_sheet_1[\\'feature\\'].isin(features1)]\\nfeature_info = pl.DataFrame(train_feature_sheet_2.reset_index())\\ndel train_feature_sheet_2, train_feature_sheet_1, feature0, features, feature1, feature_list\\n\\nwith open(\"feature_info.pkl\", \\'wb\\') as f:\\n    pickle.dump(feature_info, f)\\n\\ndir = \\'home-credit-credit-risk-model-stability\\'\\nfeature_info = pl.read_csv(\"feature_info.csv\")\\nfeature_info = feature_info.filter(pl.col(\"nan_ratio\") <= 0.7)\\nfeat_defs = pl.read_csv(os.path.join(dir0, \\'feature_definitions.csv\\'))\\nfeat_defs = feat_defs.filter(pl.col(\"Variable\").is_in(feature_info[\"feature\"]))\\ndel dir, feature_info, feat_defs\\n'"},"metadata":{}}]},{"cell_type":"code","source":"DATA_T = {\"Num\" : [\"Number\", \"number\", \"Amount\", \"amount\", \"limit\", \"Value\", \"value\", \"sum\", \"Sum\"], \n            \"Factor\": [\"Flag\", \"flag\", \"Index\", \"index\", \"Indices\", \"indices\", \"Type\", \"type\",\n                      \"Indicates\", \"indicates\", \"Status\", \"status\", \"Order\", \"Gender\", \"indicating\", \"reason\", \n                      \"Reason\", \"Classification\", \"classification\", \"Name\", \"District\", \"Zipcode\", \"address\", \"language\", \n                      \"Category\", \"category\", \"Role\", \"role\", \"Year\", \"year\"],\n            \"Ratio\": [\"Rate\", \"rate\", \"Percentage\", \"percentage\"], \n            \"Time\":  [\"Date\", \"date\"]}\n\nDATA_L = {\"Num\" : [\"Number\", \"number\", \"Amount\", \"amount\", \"DPD\", \"Days\" \"Average\", \"average\", \"limit\", \"Value\", \"value\", \n                      \"Sum\", \"sum\"], \n            \"Factor\": [\"Flag\", \"flag\", \"Index\", \"index\", \"Indices\", \"indices\", \"Type\", \"type\",\n                      \"Indicates\", \"indicates\", \"Status\", \"status\", \"Order\", \"Gender\", \"indicating\", \"reason\", \n                      \"Reason\", \"Classification\", \"classification\", \"Name\", \"District\", \"Zipcode\", \"address\", \"language\", \n                      \"Category\", \"category\", \"Role\", \"role\", \"Year\", \"year\"],\n            \"Ratio\": [\"Rate\", \"rate\", \"Percentage\", \"percentage\"],\n            \"Time\":  [\"Date\", \"date\"]}","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.870012Z","iopub.execute_input":"2024-05-13T13:25:56.870498Z","iopub.status.idle":"2024-05-13T13:25:56.882397Z","shell.execute_reply.started":"2024-05-13T13:25:56.870456Z","shell.execute_reply":"2024-05-13T13:25:56.881077Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n    \n   \n    @staticmethod\n    def set_table_dtypes(df):\n        # 读取特征定义文件\n        feat_defs = pl.read_csv(os.path.join(ROOT, 'feature_definitions.csv'))\n        # 处理特征描述列的数据类型\n        feat_defs = feat_defs.with_columns(pl.col(\"Description\").str.split(by=\" \")\n                                     .alias(\"Description\"))\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\", \"target\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64).alias(col))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n            elif col[-1] in (\"M\"):\n                df = df.with_columns(pl.col(col).cast(pl.String).alias(col))\n            elif col[-1] in (\"T\", \"L\"):\n                feat_df = feat_defs.filter(pl.col(\"Variable\") == col).select(\"Description\")\n                if not feat_df.is_empty():\n                    words = feat_df[0][\"Description\"].to_list()\n                    for word in words:\n                        if word in DATA_T[\"Num\"] or word in DATA_L[\"Num\"]:\n                            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n                        elif word in DATA_T[\"Ratio\"] or word in DATA_L[\"Ratio\"]:\n                            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n                        elif word in DATA_T[\"Factor\"] or word in DATA_L[\"Factor\"]:\n                            df = df.with_columns(pl.col(col).cast(pl.String).alias(col))\n                        elif word in DATA_T[\"Time\"] or word in DATA_L[\"Time\"]:\n                            df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n        \n        return df\n\n    @staticmethod\n    def handle_dates(df, dir_path, base_file_name):\n        \n        date_decision = pl.read_parquet(os.path.join(dir_path, base_file_name))    \\\n                               .select(pl.col(\"case_id\"), pl.col(\"date_decision\").cast(pl.Date))\n        if \"date_decision\" in df.columns and \"target\" in df.columns:\n            df.drop(\"MONTH\")\n        for col in df.columns:\n            \n            if df[col].dtype == pl.Date and col != \"date_decision\":\n                \n                # Calculate duration difference in days\n\n                df = df.join(date_decision, on='case_id', how='left')\n                diff_days_col = (pl.col(col) - pl.col(\"date_decision\")).cast(pl.Duration)\n                df = df.with_columns(diff_days_col.dt.total_days().alias(f\"{col}_diff_daysP\"))   \n                df = df.drop(\"date_decision\")\n                \n                df = df.with_columns(\n                                month_col = pl.col(col).dt.month().cast(pl.UInt8).alias(f\"month_{col}\"),\n                                weekday_col = pl.col(col).dt.weekday().cast(pl.UInt8).alias(f\"weekday_{col}\"),\n                            )\n                \n                \n        return df\n    \n          \n    @staticmethod\n    def filter_cols(df):\n        \n        for col in df.columns:\n            if col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.drop(col)\n        \n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        \n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]) \\\n                & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T14:12:58.585792Z","iopub.execute_input":"2024-05-13T14:12:58.586330Z","iopub.status.idle":"2024-05-13T14:12:58.617667Z","shell.execute_reply.started":"2024-05-13T14:12:58.586293Z","shell.execute_reply":"2024-05-13T14:12:58.616308Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if df[col].dtype == pl.datatypes.Float64]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean +expr_min \n    \n    def date_expr(df):\n        cols = [col for col in df.columns if df[col].dtype == pl.Date]\n        #expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_first +expr_last\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if df[col].dtype == pl.String]\n        # expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_last +expr_count #+expr_max\n    \n    '''\n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    '''\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.915442Z","iopub.execute_input":"2024-05-13T13:25:56.915839Z","iopub.status.idle":"2024-05-13T13:25:56.936254Z","shell.execute_reply.started":"2024-05-13T13:25:56.915808Z","shell.execute_reply":"2024-05-13T13:25:56.934931Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n        \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        #df = df.fill_null(value=\"Temp_null_value\")\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    #for col in df.columns:\n    #    df = df.with_columns(pl.col(col).cast(pl.String))\n    #    df = df.with_columns(pl.col(col).replace(df.filter(pl.col(col) == \"Temp_null_value\"),None))\n    #df = df.pipe(Pipeline.set_table_dtypes)\n    df = df.unique(subset=[\"case_id\"])\n    \n    return df\n\ndef to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    return df_data, cat_cols\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:45:38.452909Z","iopub.execute_input":"2024-05-13T15:45:38.453403Z","iopub.status.idle":"2024-05-13T15:45:38.481121Z","shell.execute_reply.started":"2024-05-13T15:45:38.453363Z","shell.execute_reply":"2024-05-13T15:45:38.479748Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"os.path.exists(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.965394Z","iopub.execute_input":"2024-05-13T13:25:56.965856Z","iopub.status.idle":"2024-05-13T13:25:56.983502Z","shell.execute_reply.started":"2024-05-13T13:25:56.965814Z","shell.execute_reply":"2024-05-13T13:25:56.982312Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTRAIN_CSV_DIR   = ROOT / \"csv_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"\nprint(TRAIN_CSV_DIR)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:25:56.985682Z","iopub.execute_input":"2024-05-13T13:25:56.986045Z","iopub.status.idle":"2024-05-13T13:25:56.996341Z","shell.execute_reply.started":"2024-05-13T13:25:56.986016Z","shell.execute_reply":"2024-05-13T13:25:56.995094Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train\n","output_type":"stream"}]},{"cell_type":"code","source":"data_store = {\n    \"df_base\": [read_file(TRAIN_DIR / \"train_base.parquet\")],\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n    ]\n}\n\nwith open('temporary_data_store.pkl', 'wb') as f:\n    pickle.dump(data_store, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:00:26.851533Z","iopub.execute_input":"2024-05-13T16:00:26.851996Z","iopub.status.idle":"2024-05-13T16:04:32.057220Z","shell.execute_reply.started":"2024-05-13T16:00:26.851960Z","shell.execute_reply":"2024-05-13T16:04:32.055883Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"'''\nWOE-Encoder:\n@Writer: BertrandBrelier\n@Copy right: MIT Liscence\n@Link: https://github.com/BertrandBrelier/woe/blob/main/woe.py\n'''\n\n# DataFrame here is based on pandas rather than polars\nclass WoeConversion():\n    def __init__(self, binarytarget, features, nbins=10):\n        self.target = binarytarget\n        self.features = features\n        self.continuousvariables = []\n        self.categoricalvariables = []\n        self.categoricalmodel = None\n        self.continuousmodel = None\n        self.nbins = nbins\n    def fit(self,df):\n        #find continuous variables:\n        traindf = df.copy(deep=True)\n        try:\n            traindf[self.target] = traindf[self.target].astype('int')\n        except:\n            print(\"ERROR : target variable must be a binary integer column with no missing values\")\n            return \n        for feat in self.features:\n            if traindf[feat].dtypes == 'O':\n                traindf[feat] = traindf[feat].astype(str)\n                self.categoricalvariables.append(feat)\n            else:\n                try:\n                    traindf[feat] = traindf[feat].astype(float)\n                    self.continuousvariables.append(feat)\n                except:\n                    self.categoricalvariables.append(feat)\n        self.categoricalmodel = ConvertCategoricalFeatures(binarytarget=self.target,CategoricalFeatures=self.categoricalvariables)\n        self.categoricalmodel.fit(traindf)\n        self.continuousmodel = ConvertContinuousFeatures(binarytarget=self.target,ContinuousFeatures=self.continuousvariables, NBins = self.nbins)\n        self.continuousmodel.fit(traindf)\n    def transform(self,testdf):\n        tmpdf = testdf.copy(deep=True)\n        tmpdf = self.categoricalmodel.transform(tmpdf)\n        tmpdf = self.continuousmodel.transform(tmpdf)\n        return tmpdf\n        \nclass ConvertCategoricalFeatures():\n    #Class to convert categorical features to WOE for binary classification problem (target = 0 or 1)\n    def __init__(self,binarytarget,CategoricalFeatures):\n        self.target = binarytarget\n        self.Model = {}\n        self.Features = CategoricalFeatures\n    def fit(self,traindf):\n        NPositive=traindf[traindf[self.target]==1].shape[0]\n        NNegative=traindf[traindf[self.target]==0].shape[0]\n        for feature in self.Features:\n            tmptraindf = traindf[[feature,self.target]].copy(deep=True)\n            tmptraindf[self.target] = tmptraindf[self.target].astype(int)\n            tmptraindf[feature] = tmptraindf[feature].astype(str)\n            results = tmptraindf[[feature,self.target]].fillna(\"None\").groupby([feature]).agg(['sum','count'])\n            results = results.reset_index()\n            results.columns=[feature,\"Positive\",\"Count\"]\n            results[\"Negative\"]=results[\"Count\"]-results[\"Positive\"]\n            results[\"CountPositive\"] = results[\"Positive\"]\n            #Replace 0 with 1 to avoid infinite log                                                                                                          \n            results.loc[results.Negative == 0, 'Negative'] = 1\n            results.loc[results.Positive == 0, 'Positive'] = 1\n            #Distribution Positive (Good)                                                                                                                    \n            results[\"DG\"]=results[\"Positive\"]*1./NPositive\n            #Distribution Negative (Bad)                                                                                                                     \n            results[\"DB\"]=results[\"Negative\"]*1./NNegative\n            #WOE                                                                                                                                             \n            results[\"WOE\"]=np.log(results[\"DG\"]/results[\"DB\"])\n            results.loc[results.Count <= 10, 'WOE'] = 0\n            results.loc[results.CountPositive <= 1, 'WOE'] = results[\"WOE\"].min()\n            results.loc[results.Count <= 10, 'WOE'] = 0\n            results = results[[feature,'WOE']]\n            self.Model[feature] = dict(zip(results[feature], results.WOE))\n    def train(self,traindf):\n        self.fit(traindf)\n    def transform(self,testdf):\n        #In case new values are found, needs to impute 0 for WOE\n        for feature in self.Features:\n            testdf[feature] = testdf[feature].astype(str)\n            testdf = testdf.fillna({feature: \"None\"})\n            ListofValues = list(set(testdf[feature].values))\n            for omega in ListofValues:\n                if omega not in self.Model[feature]:\n                    self.Model[feature][omega]=0.\n        return testdf.replace(self.Model)\n\n    \nclass ConvertContinuousFeatures():\n    #Class to convert continuous features to WOE for binary classification problem (target = 0 or 1)\n    def __init__(self,binarytarget,ContinuousFeatures,NBins):\n        self.target = binarytarget\n        self.Model = {}\n        self.Features = ContinuousFeatures\n        self.NBins = NBins\n        self.BinModel = {}\n    def train(self,traindf):\n        self.fit(traindf)\n    def fit(self,traindf):\n        NPositive=traindf[traindf[self.target]==1].shape[0]\n        NNegative=traindf[traindf[self.target]==0].shape[0]\n        for feature in self.Features:\n            tmpdf = traindf[[feature,self.target]].copy(deep=True)\n            List = sorted(list(filter(lambda x:not np.isnan(x) ,tmpdf[feature].values)))\n            Len = len(List)\n            BinsLim = [-np.inf]\n            for omega in range(1,self.NBins):\n                Value = List[int(omega * Len / self.NBins)]\n                if Value not in BinsLim:\n                    BinsLim.append( Value )\n            BinsLim.append(np.inf)\n            self.BinModel[feature] = BinsLim\n            tmpdf[\"bin\"] = pd.cut(tmpdf[feature], self.BinModel[feature], labels=range(1,len(self.BinModel[feature])))\n            tmpdf[\"bin\"] = tmpdf[\"bin\"].cat.add_categories([-1])\n            tmpdf[\"bin\"] = tmpdf[\"bin\"].fillna(-1) \n            results = tmpdf[[\"bin\",self.target]].groupby([\"bin\"]).agg(['sum','count'])\n            results = results.reset_index()\n            results.columns=[\"bin\",\"Positive\",\"Count\"]\n            results[\"Negative\"]=results[\"Count\"]-results[\"Positive\"]\n            results[\"CountPositive\"] = results[\"Positive\"]\n            #Replace 0 with 1 to avoid infinite log                                                                                                          \n            results.loc[results.Negative == 0, 'Negative'] = 1\n            results.loc[results.Positive == 0, 'Positive'] = 1\n            #Distribution Positive (Good)                                                                                                                    \n            results[\"DG\"]=results[\"Positive\"]*1./NPositive\n            #Distribution Negative (Bad)                                                                                                                     \n            results[\"DB\"]=results[\"Negative\"]*1./NNegative\n            #WOE                                                                                                                                             \n            results[\"WOE\"]=np.log(results[\"DG\"]/results[\"DB\"])\n            results.loc[results.Count <= 10, 'WOE'] = 0\n            results.loc[results.CountPositive <= 1, 'WOE'] = results[\"WOE\"].min()\n            results.loc[results.Count <= 10, 'WOE'] = 0\n            results = results[[\"bin\",'WOE']]\n            self.Model[feature] = dict(zip(results[\"bin\"], results.WOE))\n    def transform(self,testdf):\n        tmpdf = testdf.copy(deep=True)\n        for feature in self.Features:\n            tmpdf[feature] = pd.cut(tmpdf[feature], self.BinModel[feature], labels=range(1,len(self.BinModel[feature])))\n            tmpdf[feature] = tmpdf[feature].cat.add_categories([-1])\n            tmpdf[feature] = tmpdf[feature].fillna(-1) \n        return tmpdf.replace(self.Model)\n        #return tmpdf.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:29:55.333595Z","iopub.execute_input":"2024-05-13T13:29:55.334035Z","iopub.status.idle":"2024-05-13T13:29:55.385949Z","shell.execute_reply.started":"2024-05-13T13:29:55.333989Z","shell.execute_reply":"2024-05-13T13:29:55.384494Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineering(object):\n\n    def __init__(self, path_dir, based_file_name):\n        self.based_file_name = based_file_name\n        self.dir = path_dir\n    \n    def balance_dataset(self, X_features_str=\"case_id\", y_features_str=\"target\", \n                                under_ratio=0.5, over_ratio=0.5, batch_size=1000, \n                                strategy = 'ENN', output=False):\n        '''\n        过采样与欠采样解决标签不均匀的问题\n        '''\n        df = pd.read_parquet(os.path.join(self.dir, self.based_file_name))\n        X = np.array(df[X_features_str]).reshape(-1, 1)\n        y = df[y_features_str]\n        del df\n        \n        \n        # 计算目标类别分布\n        class_distribution = Counter(y)\n        majority_class = max(class_distribution, key=class_distribution.get)\n        minority_class = min(class_distribution, key=class_distribution.get)\n        \n        \n        # 分批次处理数据\n        X_resampled_batches = []\n        y_resampled_batches = []\n        for i in range(0, len(X), batch_size):\n            X_batch = X[i:i+batch_size]\n            y_batch = y[i:i+batch_size]\n            \n            # 计算欠采样和过采样的数量\n            under_sample_size_batch = min(int(class_distribution[majority_class] * under_ratio)\n                                          , len(X_batch))\n            over_sample_size_batch = min(int(class_distribution[minority_class] * over_ratio)\n                                          , len(X_batch))\n            \n\n            if strategy == 'SMOTEENN':\n                smoteenn = SMOTEENN(sampling_strategy={majority_class: under_sample_size_batch, \n                                                   minority_class: over_sample_size_batch})\n            elif strategy == 'ENN':\n                smoteenn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors = 125)\n            X_resampled_batch, y_resampled_batch = smoteenn.fit_resample(X_batch, y_batch)\n            \n            X_resampled_batches.append(X_resampled_batch)\n            y_resampled_batches.append(y_resampled_batch)\n            del X_resampled_batch, y_resampled_batch, X_batch, y_batch\n        \n        # 合并批次\n        self.X_resampled = np.concatenate(X_resampled_batches)\n        self.y_resampled = np.concatenate(y_resampled_batches)\n        del X_resampled_batches, y_resampled_batches\n        \n        \n        # 导出csv\n        if output:\n            df_resample = pd.DataFrame({X_features_str: self.X_resampled.flatten(), y_features_str: self.y_resampled})\n            df_resample.to_csv(\"train_base_resample.csv\", index=False)\n        else:\n            return self.X_resampled, self.y_resampled\n    \n    \n    def encoder(self, df):\n        '''\n        特征编码器\n        '''\n        cols_woe = []\n        for col in df.columns:\n            if col in [\"case_id\"]:\n                pass\n            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"max_num_group1\", \\\n                         \"last_num_group1\", \"max_num_group2\", \"last_num_group2\"]:\n                df.drop(pl.col(col))\n            elif col in [\"date_decision\"]:\n                pass\n            elif df[col].dtype in [pl.Date]:\n                         \n                df = df.with_columns(\n                                month_col = pl.col(col).dt.month().alias(f\"month_{col}\"),\n                                weekday_col = pl.col(col).dt.weekday().alias(f\"weekday_{col}\"),\n                                month_col_str = pl.col(f\"month_{col}\").dt.month().cast(pl.Utf8).alias(f\"weekday_{col}\"), \n                                weekday_col_str  = pl.col(f\"weekday_{col}\").dt.weekday().cast(pl.Utf8).alias(f\"weekday_{col}\")\n                            )\n                df.drop(col)\n                cols_woe.append(f\"month_{col}\", f\"weekday_{col}\")\n                         \n            elif df[col].dtype in [pl.datatypes.Int64, pl.datatypes.Int32, pl.datatypes.Float64, pl.datatypes.String]:\n                #Encoding\n                cols_woe.append(col)\n                         \n        woemodel = WoeConversion(binarytarget='target', features=cols_woe)\n        woemodel.fit(df)\n        df = woemodel.transform(df)\n        \n        with open(\"encoder.pkl\", \"wb\") as f:\n            pickle.dump(woemodel, f) #需要时直接调用模型\n                         \n        return df  \n        \n        \n        \n    def df_data_compaction(self, df, base_case_ids):\n                \n        if not isinstance(base_case_ids, list):\n            base_case_ids = list(base_case_ids.reshape((-1,)))\n        df = df.filter(\n                pl.col(\"case_id\").is_in(base_case_ids))\n        df = df.pipe(Pipeline.handle_dates, self.dir, self.based_file_name) #传参可能有bug\n        # df = df.pipe(Pipeline.filter_cols)\n        return df \n    \n    \n    def df_concatenates(self, data_store_dict):\n        \n        # 同一特征是否有重复合并之嫌\n        feature_processed = []\n        X_train = data_store_dict[\"df_base\"][0]\n        for key, values in data_store_dict.items():\n            print(\"Files is being concatenated: \", key)\n            for value in values:\n                # print(\"Type of value is \", type(value))\n                feature_processing = []\n                for column_name in value.columns:       \n                    # justification\n                    if column_name in feature_processed and not column_name in \\\n                    [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\", \"max_num_group1\", \"last_num_group1\", \\\n                     \"max_num_group2\", \"last_num_group2\", \"target\"]:\n                        \n                        X_train = X_train.join(value.select(pl.col(\"case_id\"), pl.col(column_name)), \n                                                 on=\"case_id\", how=\"outer_coalesce\")\n\n                        X_train = X_train.with_columns(\n                                    pl.when(pl.col(column_name).is_not_null()).then(pl.col(column_name)) \\\n                                      .otherwise(pl.col(f\"{column_name}_right\")).alias(column_name) \n                            ).drop(f\"{column_name}_right\")\n                    elif column_name in \\\n                    [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"max_num_group1\", \"last_num_group1\", \\\n                     \"max_num_group2\", \"last_num_group2\"]:\n                        pass\n                    elif column_name == \"case_id\":\n                        feature_processing.append(column_name)\n                    elif column_name == \"target\":\n                        pass\n                    else: \n                        feature_processing.append(column_name)\n                        feature_processed.append(column_name)\n                 \n                #按照case_id左连接\n                X_train = X_train.join(value.select(feature_processing), on='case_id', how=\"outer_coalesce\")\n                del feature_processing\n        del feature_processed\n        gc.collect()\n        \n        return X_train                          \n    \n          \n        \n    def main(self):\n    \n    \n        df_base = read_file(os.path.join(self.dir, self.based_file_name))\n        case_ids, _ = self.balance_dataset()\n        case_ids = list(case_ids.reshape((-1,)))\n        del _\n        \n        \n        with open('temporary_data_store.pkl', 'rb') as f:\n            data_dict = {}\n            while True:\n                try:\n                    data_loaded = pickle.load(f)\n                    for key, value in data_loaded.items():\n                        print(\"Processing files: \", key)\n                        if key == 'df_base':\n                            \n                            \n                            #重采样减少数据量\n                            value = value[0]\n                            df_base = df_base.filter(df_base['case_id'].is_in(case_ids)) \n                            df_base = df_base.with_columns(\n                                month_decision = pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n                                weekday_decision = pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n                            )\n                            data_dict.update({key: [df_base]})\n                            target = df_base[\"target\"]\n                            del df_base\n                            \n                        elif key == 'depth_0' or 'depth_1' or 'depth_2':\n                            #并行化处理函数\n\n                            from concurrent.futures import ThreadPoolExecutor\n                            # 创建一个临时字典来存储处理后的 DataFrame\n                            temp_value = {}\n\n                            with ThreadPoolExecutor(max_workers=3) as executor:\n                                # 对 depth_* 中的每个 DataFrame 进行并行处理\n                                futures = {i: executor.submit(self.df_data_compaction, df, case_ids) for i, df in enumerate(value)}\n\n                                # 等待所有任务完成\n                                for i, future in futures.items():\n                                    # 获取任务的结果\n                                    filtered_df = future.result()\n                                    # 将处理后的 DataFrame 存储到临时字典中\n                                    temp_value[i] = filtered_df\n                                \n                            data_dict.update({key: list(temp_value.values())})\n                        else:\n                            data_dict.update({key: value})\n                        print(\"Files processed: \", key)\n                        \n                        \n                except EOFError:\n                    # 到达文件末尾\n                    break\n        \n        #合并\n        train_data = self.df_concatenates(data_dict)\n        #del data_dict\n        #初步特征选择\n        train_data = train_data.pipe(Pipeline.filter_cols)\n        train_data = pd.DataFrame(train_data, columns = train_data.columns)\n        #编码\n        train_data = self.encoder(train_data)\n\n        # 将数据重新保存到 pkl 文件中\n        with open('temporary_data_store.pkl', 'wb') as f:\n            pickle.dump(data_dict, f)\n      ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:29:55.388465Z","iopub.execute_input":"2024-05-13T13:29:55.388937Z","iopub.status.idle":"2024-05-13T13:29:56.470256Z","shell.execute_reply.started":"2024-05-13T13:29:55.388897Z","shell.execute_reply":"2024-05-13T13:29:56.468706Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class FeatureEngineering(object):\n\n    def __init__(self, path_dir, based_file_name):\n        self.based_file_name = based_file_name\n        self.dir = path_dir\n    \n    def balance_dataset(self, X_features_str=\"case_id\", y_features_str=\"target\", \n                                under_ratio=0.5, over_ratio=0.5, batch_size=1000, \n                                strategy = 'ENN', output=False):\n        '''\n        过采样与欠采样解决标签不均匀的问题\n        '''\n        df = pd.read_parquet(os.path.join(self.dir, self.based_file_name))\n        X = np.array(df[X_features_str]).reshape(-1, 1)\n        y = df[y_features_str]\n        del df\n        \n        \n        # 计算目标类别分布\n        class_distribution = Counter(y)\n        majority_class = max(class_distribution, key=class_distribution.get)\n        minority_class = min(class_distribution, key=class_distribution.get)\n        \n        \n        # 分批次处理数据\n        X_resampled_batches = []\n        y_resampled_batches = []\n        for i in range(0, len(X), batch_size):\n            X_batch = X[i:i+batch_size]\n            y_batch = y[i:i+batch_size]\n            \n            # 计算欠采样和过采样的数量\n            under_sample_size_batch = min(int(class_distribution[majority_class] * under_ratio)\n                                          , len(X_batch))\n            over_sample_size_batch = min(int(class_distribution[minority_class] * over_ratio)\n                                          , len(X_batch))\n            \n\n            if strategy == 'SMOTEENN':\n                smoteenn = SMOTEENN(sampling_strategy={majority_class: under_sample_size_batch, \n                                                   minority_class: over_sample_size_batch})\n            elif strategy == 'ENN':\n                smoteenn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors = 125)\n            X_resampled_batch, y_resampled_batch = smoteenn.fit_resample(X_batch, y_batch)\n            \n            X_resampled_batches.append(X_resampled_batch)\n            y_resampled_batches.append(y_resampled_batch)\n            del X_resampled_batch, y_resampled_batch, X_batch, y_batch\n        \n        # 合并批次\n        self.X_resampled = np.concatenate(X_resampled_batches)\n        self.y_resampled = np.concatenate(y_resampled_batches)\n        del X_resampled_batches, y_resampled_batches\n        \n        \n        # 导出csv\n        if output:\n            df_resample = pd.DataFrame({X_features_str: self.X_resampled.flatten(), y_features_str: self.y_resampled})\n            df_resample.to_csv(\"train_base_resample.csv\", index=False)\n        else:\n            return self.X_resampled, self.y_resampled\n    \n    \n    def encoder(self, df):\n        cols_woe = []\n        polars_int = [pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Int64, pl.Int32, pl.Int8, pl.Int16]\n        polars_str = [pl.String, str, object, pl.Boolean]\n        polars_flt = [pl.Float32, pl.Float64]\n        \n        for col in df.columns:\n            if col == \"case_id\":\n                continue  # 跳过特定列 \"case_id\"\n            elif col == \"target\":\n                df = df.with_columns(pl.col(col).cast(pl.Int32))  # 将 \"target\" 列转换为 Int32 类型\n            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"MONTH\", \"date_decision\"]:\n                df = df.drop(col)  # 删除特定列\n            elif df[col].dtype in polars_int or df[col].dtype in polars_flt:\n                # 填充数值类型列的空值为平均值\n                cols_woe.append(col)\n                df = df.with_columns(pl.col(col).fill_null(pl.col(col).mean()).alias(col))\n            elif df[col].dtype in polars_str:\n                # 填充字符串类型列的空值为 \"NA\"\n                cols_woe.append(col)\n                df = df.with_columns(pl.col(col).fill_null(value=\"NA\").alias(col))\n\n        woemodel = WoeConversion(binarytarget='target', features=cols_woe)\n        tuples_list = zip(df.columns, df.dtypes)\n        dtype_dict = {k: v for k, v in tuples_list}\n        \n        # 将 pl.DataType 映射为 python datatype\n        dtype_dict = {k: (int if v in polars_int else v) for k, v in dtype_dict.items()}\n        dtype_dict = {k: (str if v in polars_str else v) for k, v in dtype_dict.items()}\n        dtype_dict = {k: (float if v in polars_flt else v) for k, v in dtype_dict.items()}\n        \n        #这里要先处理缺失值\n        df_pd = pd.DataFrame(df, columns=df.columns).astype(dtype_dict)\n        woemodel.fit(df_pd)\n        df_pd = woemodel.transform(df_pd)\n        with open(\"encoder.pkl\", \"wb\") as f:\n            pickle.dump(woemodel, f) #需要时直接调用模型\n                         \n        return df_pd  \n        \n        \n        \n    def df_data_compaction(self, df, base_case_ids):\n                \n        if not isinstance(base_case_ids, list):\n            base_case_ids = list(base_case_ids.reshape((-1,)))\n        df = df.filter(\n                pl.col(\"case_id\").is_in(base_case_ids))\n        df = df.pipe(Pipeline.handle_dates, self.dir, self.based_file_name) #传参可能有bug\n        # df = df.pipe(Pipeline.filter_cols)\n        return df \n    \n    \n    def df_concatenates(self, data_store_dict):\n        \n        # 同一特征是否有重复合并之嫌\n        feature_processed = []\n        X_train = data_store_dict[\"df_base\"][0]\n        for key, values in data_store_dict.items():\n            for value in values:\n                feature_processing = []\n                for column_name in value.columns:       \n                    # justification\n                    if column_name in feature_processed and not column_name in data_store_dict[\"df_base\"][0].columns:\n                        '''\n                                                                               [\"case_id\", \"target\", \"WEEK_NUM\", \\\n                                                                                \"num_group1\", \"num_group2\", \"max_num_group1\", \\\n                                                                                \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n                                                                                \"date_decision\", \"weekday_decision\", \"month_decision\", \\\n                                                                                \"MONTH\"]:\n                       '''\n                        \n                        new_column = X_train.join(value.select(pl.col(\"case_id\"), pl.col(column_name)), \n                                                 on=\"case_id\", how=\"outer_coalesce\")\n\n                        new_column = new_column.with_columns(\n                                    pl.when(pl.col(column_name).is_not_null()).then(pl.col(column_name)) \\\n                                      .otherwise(pl.col(f\"{column_name}_right\")).alias(column_name) \n                            ).drop(f\"{column_name}_right\")\n                    elif column_name == \"case_id\":\n                        feature_processing.append(column_name)\n                    elif column_name == \"target\":\n                        pass\n                    elif column_name in data_store_dict[\"df_base\"][0].columns:\n                        '''\n                                        [\"WEEK_NUM\", \"num_group1\", \"num_group2\",\\\n                                         \"max_num_group1\", \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n                                         \"date_decision\", \"weekday_decision\", \"month_decision\", \"MONTH\"]:\n                        '''\n                        pass\n                    elif value[column_name].dtype == pl.Date:\n                        pass\n                    else: \n                        feature_processing.append(column_name)\n                        feature_processed.append(column_name)\n                 \n                #按照case_id左连接\n                X_train = X_train.join(value.select(feature_processing), on='case_id', how=\"outer_coalesce\")\n                del feature_processing\n        del feature_processed\n        gc.collect()\n        \n        return X_train                          \n    \n          \n        \n    def main(self):\n    \n    \n        df_base = read_file(os.path.join(self.dir, self.based_file_name))\n        case_ids, _ = self.balance_dataset()\n        case_ids = list(case_ids.reshape((-1,)))\n        del _\n        \n        \n        with open('temporary_data_store.pkl', 'rb') as f:\n            data_dict = {}\n            while True:\n                try:\n                    data_loaded = pickle.load(f)\n                    for key, value in data_loaded.items():\n                        print(\"Processing files: \", key)\n                        if key == 'df_base':\n                            \n                            \n                            #重采样减少数据量\n                            value = value[0]\n                            df_base = df_base.filter(df_base['case_id'].is_in(case_ids)) \n                            df_base = df_base.with_columns(\n                                month_decision = pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n                                weekday_decision = pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n                            )\n                            data_dict.update({key: [df_base]})\n                            target = df_base[\"target\"]\n                            del df_base\n                            \n                        elif key == 'depth_0' or 'depth_1' or 'depth_2':\n                            #并行化处理函数\n\n                            from concurrent.futures import ThreadPoolExecutor\n                            # 创建一个临时字典来存储处理后的 DataFrame\n                            temp_value = {}\n\n                            with ThreadPoolExecutor(max_workers=3) as executor:\n                                # 对 depth_* 中的每个 DataFrame 进行并行处理\n                                futures = {i: executor.submit(self.df_data_compaction, df, case_ids) for i, df in enumerate(value)}\n\n                                # 等待所有任务完成\n                                for i, future in futures.items():\n                                    # 获取任务的结果\n                                    filtered_df = future.result()\n                                    # 将处理后的 DataFrame 存储到临时字典中\n                                    temp_value[i] = filtered_df\n                                    del filtered_df\n\n                            data_dict.update({key: list(temp_value.values())})\n                        else:\n                            data_dict.update({key: value})\n                        print(\"Files processed: \", key)\n                        \n                        \n                except EOFError:\n                    # 到达文件末尾\n                    break\n        \n        #合并\n        train_data = self.df_concatenates(data_dict)\n        del data_dict\n        #初步特征选择\n        train_data = train_data.pipe(Pipeline.filter_cols)\n        #编码\n        train_data = self.encoder(train_data)\n\n        # 将数据重新保存到 pkl 文件中\n        with open('train_data_store.pkl', 'wb') as f:\n            pickle.dump(train_data, f) ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:32:38.299831Z","iopub.execute_input":"2024-05-13T13:32:38.300326Z","iopub.status.idle":"2024-05-13T13:32:38.368089Z","shell.execute_reply.started":"2024-05-13T13:32:38.300288Z","shell.execute_reply":"2024-05-13T13:32:38.366563Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class TestFeatureEngineering(object):\n\n    \n    def __init__(self, path_dir, based_file_name):\n        self.based_file_name = based_file_name\n        self.dir = path_dir\n        \n        \n    def df_data_compaction(self, df, base_case_ids):\n                \n        if not isinstance(base_case_ids, list):\n            base_case_ids = list(base_case_ids.reshape((-1,)))\n        df = df.filter(\n                pl.col(\"case_id\").is_in(base_case_ids))\n        df = df.pipe(Pipeline.handle_dates, self.dir, self.based_file_name) #传参可能有bug\n        # df = df.pipe(Pipeline.filter_cols)\n        return df \n    \n    \n    def df_concatenates(self, data_store_dict):\n        \n        # 同一特征是否有重复合并之嫌\n        feature_processed = []\n        X_train = data_store_dict[\"df_base\"][0]\n        for key, values in data_store_dict.items():\n            for value in values:\n                feature_processing = []\n                for column_name in value.columns:       \n                    # justification\n                    if column_name in feature_processed and not column_name in data_store_dict[\"df_base\"][0].columns:\n                        '''\n                                                                               [\"case_id\", \"target\", \"WEEK_NUM\", \\\n                                                                                \"num_group1\", \"num_group2\", \"max_num_group1\", \\\n                                                                                \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n                                                                                \"date_decision\", \"weekday_decision\", \"month_decision\", \\\n                                                                                \"MONTH\"]:\n                       '''\n                        \n                        new_column = X_train.join(value.select(pl.col(\"case_id\"), pl.col(column_name)), \n                                                 on=\"case_id\", how=\"outer_coalesce\")\n\n                        new_column = new_column.with_columns(\n                                    pl.when(pl.col(column_name).is_not_null()).then(pl.col(column_name)) \\\n                                      .otherwise(pl.col(f\"{column_name}_right\")).alias(column_name) \n                            ).drop(f\"{column_name}_right\")\n                    elif column_name == \"case_id\":\n                        feature_processing.append(column_name)\n                    elif column_name == \"target\":\n                        pass\n                    elif column_name in data_store_dict[\"df_base\"][0].columns:\n                        '''\n                                        [\"WEEK_NUM\", \"num_group1\", \"num_group2\",\\\n                                         \"max_num_group1\", \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n                                         \"date_decision\", \"weekday_decision\", \"month_decision\", \"MONTH\"]:\n                        '''\n                        pass\n                    elif value[column_name].dtype == pl.Date:\n                        pass\n                    else: \n                        feature_processing.append(column_name)\n                        feature_processed.append(column_name)\n                 \n                #按照case_id左连接\n                X_train = X_train.join(value.select(feature_processing), on='case_id', how=\"outer_coalesce\")\n                del feature_processing\n        del feature_processed\n        gc.collect()\n        \n        return X_train\n    \n    \n    def encoding_transform(self, df, encoder_fitted):\n        cols_woe = []\n        polars_int = [pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Int64, pl.Int32, pl.Int8, pl.Int16]\n        polars_str = [pl.String, str, object, pl.Boolean]\n        polars_flt = [pl.Float32, pl.Float64]\n        \n        for col in df.columns:\n            if col == \"case_id\":\n                continue  # 跳过特定列 \"case_id\"\n            elif col == \"target\":\n                df = df.with_columns(pl.col(col).cast(pl.Int32))  # 将 \"target\" 列转换为 Int32 类型\n            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"MONTH\", \"date_decision\"]:\n                df = df.drop(col)  # 删除特定列\n            elif df[col].dtype in polars_int or df[col].dtype in polars_flt:\n                # 填充数值类型列的空值为平均值\n                cols_woe.append(col)\n                df = df.with_columns(pl.col(col).fill_null(pl.col(col).mean()).alias(col))\n            elif df[col].dtype in polars_str:\n                # 填充字符串类型列的空值为 \"NA\"\n                cols_woe.append(col)\n                df = df.with_columns(pl.col(col).fill_null(value=\"NA\").alias(col))\n\n        tuples_list = zip(df.columns, df.dtypes)\n        dtype_dict = {k: v for k, v in tuples_list}\n        \n        # 将 pl.DataType 映射为 python datatype\n        dtype_dict = {k: (int if v in polars_int else v) for k, v in dtype_dict.items()}\n        dtype_dict = {k: (str if v in polars_str else v) for k, v in dtype_dict.items()}\n        dtype_dict = {k: (float if v in polars_flt else v) for k, v in dtype_dict.items()}\n        \n        #这里要先处理缺失值\n        df_pd = pd.DataFrame(df, columns=df.columns).astype(dtype_dict)\n        \n        df_pd = encoder_fitted.transform(df_pd)\n        return df_pd\n    \n\n    def main(self, encoder_fitted):\n    \n    \n        df_base = read_file(os.path.join(self.dir, self.based_file_name))\n        case_ids = df_base.columns\n        \n        with open('temporary_data_store.pkl', 'rb') as f:\n            data_dict = {}\n            while True:\n                try:\n                    data_loaded = pickle.load(f)\n                    for key, value in data_loaded.items():\n                        print(\"Processing files: \", key)\n                        if key == 'df_base':\n                            \n                            value = value[0]\n                            df_base = df_base.filter(df_base['case_id'].is_in(case_ids)) \n                            df_base = df_base.with_columns(\n                                month_decision = pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n                                weekday_decision = pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n                            )\n                            data_dict.update({key: [df_base]})\n                            del df_base\n                            \n                        elif key == 'depth_0' or 'depth_1' or 'depth_2':\n                            #并行化处理函数\n\n                            from concurrent.futures import ThreadPoolExecutor\n                            # 创建一个临时字典来存储处理后的 DataFrame\n                            temp_value = {}\n\n                            with ThreadPoolExecutor(max_workers=3) as executor:\n                                # 对 depth_* 中的每个 DataFrame 进行并行处理\n                                futures = {i: executor.submit(self.df_data_compaction, df, case_ids) for i, df in enumerate(value)}\n\n                                # 等待所有任务完成\n                                for i, future in futures.items():\n                                    # 获取任务的结果\n                                    filtered_df = future.result()\n                                    # 将处理后的 DataFrame 存储到临时字典中\n                                    temp_value[i] = filtered_df\n                                    del filtered_df\n\n                            data_dict.update({key: list(temp_value.values())})\n                        else:\n                            data_dict.update({key: value})\n                        print(\"Files processed: \", key)\n                        \n                        \n                except EOFError:\n                    # 到达文件末尾\n                    break\n        \n        #合并\n        test_data = self.df_concatenates(data_dict)\n        del data_dict\n        #初步特征选择\n        with open('train_data_store.pkl', 'rb') as f:\n            train_data = pickle.load(train_data, f) \n        test_data = test_data.filter(train_data.columns)\n        del train_data\n        #编码\n        test_data = self.encoding_transform(test_data, encoder_fitted)\n\n        # 将数据重新保存到 pkl 文件中\n        with open('test_data_store.pkl', 'wb') as f:\n            pickle.dump(test_data, f) ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:38:44.593907Z","iopub.execute_input":"2024-05-13T13:38:44.594339Z","iopub.status.idle":"2024-05-13T13:38:44.638833Z","shell.execute_reply.started":"2024-05-13T13:38:44.594308Z","shell.execute_reply":"2024-05-13T13:38:44.637586Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#del data_store \ngc.collect()\nFeatureEngineering(path_dir= TRAIN_DIR, based_file_name=\"train_base.parquet\").main()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:32:54.376584Z","iopub.execute_input":"2024-05-13T13:32:54.378032Z","iopub.status.idle":"2024-05-13T13:37:39.601482Z","shell.execute_reply.started":"2024-05-13T13:32:54.377987Z","shell.execute_reply":"2024-05-13T13:37:39.600077Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Processing files:  df_base\nFiles processed:  df_base\nProcessing files:  depth_0\nFiles processed:  depth_0\nProcessing files:  depth_1\nFiles processed:  depth_1\nProcessing files:  depth_2\nFiles processed:  depth_2\n","output_type":"stream"}]},{"cell_type":"code","source":"data_store = {\n    \"df_base\": [read_file(TEST_DIR / \"test_base.parquet\")],\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        Test.read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}\n\nwith open('temporary_data_store.pkl', 'wb') as f:\n    pickle.dump(data_store, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:59:53.623844Z","iopub.execute_input":"2024-05-13T15:59:53.624632Z","iopub.status.idle":"2024-05-13T15:59:54.020245Z","shell.execute_reply.started":"2024-05-13T15:59:53.624594Z","shell.execute_reply":"2024-05-13T15:59:54.018506Z"},"trusted":true},"execution_count":75,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m data_store \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_base\u001b[39m\u001b[38;5;124m\"\u001b[39m: [read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_base.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_static_cb_0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m         read_files(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_static_0_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m----> 8\u001b[0m         \u001b[43mTest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_applprev_1_*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      9\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_tax_registry_a_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     10\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_tax_registry_b_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_tax_registry_c_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m         read_files(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_a_1_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_b_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     14\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_other_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     15\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_person_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     16\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_deposit_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     17\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_debitcard_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m     ],\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     20\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_b_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     21\u001b[0m         read_files(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_a_2_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     22\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_applprev_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     23\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_person_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporary_data_store.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     28\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(data_store, f)\n","Cell \u001b[0;32mIn[74], line 15\u001b[0m, in \u001b[0;36mTest.read_files\u001b[0;34m(regex_path, depth)\u001b[0m\n\u001b[1;32m     12\u001b[0m             df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(col)\u001b[38;5;241m.\u001b[39mfill_null(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemp_null_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(col))\n\u001b[1;32m     13\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvertical_relaxed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     17\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(col)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mString))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/polars/functions/eager.py:187\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(items, how, rechunk, parallel)\u001b[0m\n\u001b[1;32m    184\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_df(plr\u001b[38;5;241m.\u001b[39mconcat_df(elems))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical_relaxed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_ldf(\n\u001b[0;32m--> 187\u001b[0m         \u001b[43mplr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_lf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mto_supertypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     )\u001b[38;5;241m.\u001b[39mcollect(no_optimization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagonal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    196\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_df(plr\u001b[38;5;241m.\u001b[39mconcat_df_diagonal(elems))\n","\u001b[0;31mComputeError\u001b[0m: schema lengths differ"],"ename":"ComputeError","evalue":"schema lengths differ","output_type":"error"}]},{"cell_type":"code","source":"with open(\"encoder.pkl\", \"rb\") as f:\n    woe_encoder = pickle.load(f) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del data_store \ngc.collect()\nTestFeatureEngineering(path_dir= TEST_DIR, based_file_name=\"test_base.parquet\").main(woe_encoder)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:31:33.252312Z","iopub.status.idle":"2024-05-13T13:31:33.252940Z","shell.execute_reply.started":"2024-05-13T13:31:33.252630Z","shell.execute_reply":"2024-05-13T13:31:33.252657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test:  \n\n    def read_files(regex_path, depth=None):\n        chunks = []\n    \n        for path in glob(str(regex_path)):\n            df = pl.read_parquet(path)\n            df = df.pipe(Pipeline.set_table_dtypes)\n            if depth in [1, 2]:\n                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n                for col in df.columns:\n                    df = df.with_columns(pl.col(col).fill_null(value=\"0\").alias(col))\n            chunks.append(df)\n    \n        df = pl.concat(chunks, how=\"vertical_relaxed\")\n        for col in df.columns:\n            df = df.with_columns(pl.col(col).cast(pl.String))\n            df = df.with_columns(pl.col(col).replace(df.filter(pl.col(col) == \"Temp_null_value\"),None).alias(col))\n        df = df.pipe(Pipeline.set_table_dtypes)\n        df = df.unique(subset=[\"case_id\"])\n    \n        return df\n            \n            \n    def encoder(df):\n        cols_woe = []\n        polars_int = [pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Int64, pl.Int32, pl.Int8, pl.Int16]\n        polars_str = [pl.String, str, object, pl.Boolean]\n        polars_flt = [pl.Float32, pl.Float64]\n        \n        for col in df.columns:\n            if col == \"case_id\":\n                continue  # 跳过特定列 \"case_id\"\n            elif col == \"target\":\n                df = df.with_columns(pl.col(col).cast(pl.Int32))  # 将 \"target\" 列转换为 Int32 类型\n            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"MONTH\", \"date_decision\"]:\n                df = df.drop(col)  # 删除特定列\n            elif df[col].dtype in polars_int or df[col].dtype in polars_flt:\n                # 填充数值类型列的空值为平均值\n                cols_woe.append(col)\n                df = df.with_columns(pl.col(col).fill_null(pl.col(col).mean()).alias(col))\n            elif df[col].dtype in polars_str:\n                # 填充字符串类型列的空值为 \"NA\"\n                cols_woe.append(col)\n                df = df.with_columns(pl.col(col).fill_null(value=\"NA\").alias(col))\n\n        woemodel = WoeConversion(binarytarget='target', features=cols_woe)\n        tuples_list = zip(df.columns, df.dtypes)\n        dtype_dict = {k: v for k, v in tuples_list}\n        \n        # 将 pl.DataType 映射为 python datatype\n        dtype_dict = {k: (int if v in polars_int else v) for k, v in dtype_dict.items()}\n        dtype_dict = {k: (str if v in polars_str else v) for k, v in dtype_dict.items()}\n        dtype_dict = {k: (float if v in polars_flt else v) for k, v in dtype_dict.items()}\n        \n        #这里要先处理缺失值\n        df_pd = pd.DataFrame(df, columns=df.columns).astype(dtype_dict)\n        \n        woemodel.fit(df_pd)\n        df_pd = woemodel.transform(df_pd)\n        return df_pd","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:59:50.399426Z","iopub.execute_input":"2024-05-13T15:59:50.399862Z","iopub.status.idle":"2024-05-13T15:59:50.422553Z","shell.execute_reply.started":"2024-05-13T15:59:50.399829Z","shell.execute_reply":"2024-05-13T15:59:50.421010Z"},"trusted":true},"execution_count":74,"outputs":[]}]}